>>> Evaluating Bert
-> Role: 2
-> Address: 127.0.0.1
-> Port: 8000
<<<

> Setup Linear
> HE instance initialized: 
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55 
-> Plaintext Mod: 536903681(29 bits)

> HE instance initialized: 
-> Poly Mod Degree: 8192
-> Coeff Mod: 60 60 60 
-> Plaintext Mod: 557057(19 bits)

> HE instance initialized: 
-> Poly Mod Degree: 8192
-> Coeff Mod: 54 54 55 55 
-> Plaintext Mod: 4295049217(32 bits)

> Setup NonLinear
> Bert intialized done!

==>> Inference sample #0
> Loading inputs
> Repacking to column
> Send to client
> --- Entering Attention Layers ---
-> Sharing Pooling and Classification params...
-> Layer - Pooling
-> Layer - Classification
> [TIMING]: linear1 takes 175.073 sec
> [TIMING]: linear2 takes 17.6415 sec
> [TIMING]: linear3 takes 50.9398 sec
> [TIMING]: linear4 takes 52.6039 sec
> [TIMING]: softmax takes 869.775 sec
> [TIMING]: pruning takes 0.00864359 sec
> [TIMING]: mul v takes 62.4982 sec
> [TIMING]: gelu takes 768.622 sec
> [TIMING]: ln_1 takes 555.167 sec
> [TIMING]: ln_2 takes 553.386 sec
> [TIMING]: tanh takes 13.6207 sec
> [TIMING]: repacking takes 16.3287 sec
> [TIMING]: gt_sub takes 298.752 sec
> [TIMING]: shift takes 222.014 sec
> [TIMING]: conversion takes 374.893 sec
> [TIMING]: ln_share takes 0.0127921 sec
> [TIMING]: Pool/Class takes 45.7532 sec
> [NETWORK]: Linear 1 consumes: 50.8319 Mbytes
> [NETWORK]: Linear 2 consumes: 36.0191 Mbytes
> [NETWORK]: Linear 3 consumes: 36.0191 Mbytes
> [NETWORK]: Linear 4 consumes: 144.076 Mbytes
> [NETWORK]: Softmax consumes: 8685.88 Mbytes
> [NETWORK]: GELU consumes: 8830.03 Mbytes
> [NETWORK]: Layer Norm 1 consumes: 3613.35 Mbytes
> [NETWORK]: Layer Norm 2 consumes: 3613.35 Mbytes
> [NETWORK]: Tanh consumes: 8.32031 Mbytes
> [NETWORK]: Softmax * V: 101.664 Mbytes
> [NETWORK]: Pruning: 0 Mbytes
> [NETWORK]: Shift consumes: 2007.79 Mbytes
> [NETWORK]: gt_sub consumes: 3225.47 Mbytes
> [NETWORK]: Pooling / C consumes: 292.652 Mbytes
> [NETWORK]: Linear 1 consumes: 23 rounds
> [NETWORK]: Linear 2 consumes: 24 rounds
> [NETWORK]: Linear 3 consumes: 24 rounds
> [NETWORK]: Linear 4 consumes: 24 rounds
> [NETWORK]: Softmax consumes: 89088 rounds
> [NETWORK]: GELU consumes: 33792 rounds
> [NETWORK]: Layer Norm 1 consumes: 83736 rounds
> [NETWORK]: Layer Norm 2 consumes: 83736 rounds
> [NETWORK]: Tanh consumes: 3520 rounds
> [NETWORK]: Softmax * V: 24 rounds
> [NETWORK]: Pruning: 0 rounds
> [NETWORK]: Shift consumes: 18368 rounds
> [NETWORK]: gt_sub consumes: 16864 rounds
> [NETWORK]: Pooling / C consumes: 988 rounds
-> End to end takes: 3688.62sec
